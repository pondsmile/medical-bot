from flask import Flask, request, abort
from linebot.v3.messaging import (
    Configuration, ApiClient, MessagingApi, ReplyMessageRequest,
    PushMessageRequest, TextMessage
)
from linebot.v3.webhooks import MessageEvent, TextMessageContent
from linebot.v3.webhook import WebhookHandler
from linebot.v3.exceptions import InvalidSignatureError
import os
import PyPDF2
import re
from google import genai
from google.genai import types
import logging
import json
import vertexai

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# LINE Credentials
LINE_CHANNEL_SECRET = '5c8d2c4b41d6307b5df9f14ca01bb1df'
LINE_CHANNEL_ACCESS_TOKEN = 'S57WX85ldXlMWLDXqYMgwAFRFVsQ9PYjIzp5Ov8d1YV+U6yHvClLTqDAsUx2XZ8zMJdNlyztCGX+Qd+r9Hjaw4wYliM3cU4er6H57nlhIhE1mwSYDobeic8pk3igOO1JWhy/3TJd7iu9icbEeWrsdQdB04t89/1O/w1cDnyilFU='

# Initialize Vertex AI
vertexai.init(project="lexical-period-444405-e3", location="us-central1")


class PDFContentManager:
    def __init__(self, pdf_folder="data"):
        self.pdf_folder = pdf_folder
        self.document_categories = {}
        self.content_sections = []
        self.load_or_create_content()

    def categorize_document(self, text, filename):
        text_lower = text.lower()
        categories = []

        if any(word in text_lower for word in ['‡∏Ñ‡πà‡∏≤‡∏£‡∏±‡∏Å‡∏©‡∏≤', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', '‡∏≠‡∏±‡∏ï‡∏£‡∏≤']):
            categories.append('pricing')
        if any(word in text_lower for word in ['‡πÇ‡∏£‡∏Ñ', '‡∏≠‡∏≤‡∏Å‡∏≤‡∏£', '‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤', '‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢']):
            categories.append('medical')
        if any(word in text_lower for word in ['‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô', '‡∏ß‡∏¥‡∏ò‡∏µ', '‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£']):
            categories.append('procedure')
        if any(word in text_lower for word in ['‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢', '‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö', '‡∏Ç‡πâ‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö']):
            categories.append('policy')

        if not categories:
            categories.append('general')

        return categories

    def extract_section_metadata(self, section):
        metadata = {
            'type': 'unknown',
            'keywords': set(),
            'numbers': [],
            'has_pricing': False
        }

        text_lower = section.lower()

        if any(word in text_lower for word in ['‡∏ö‡∏≤‡∏ó', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏Ñ‡πà‡∏≤']):
            metadata['type'] = 'pricing'
            metadata['has_pricing'] = True
        elif any(word in text_lower for word in ['‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô', '‡∏ß‡∏¥‡∏ò‡∏µ']):
            metadata['type'] = 'procedure'
        elif any(word in text_lower for word in ['‡πÇ‡∏£‡∏Ñ', '‡∏≠‡∏≤‡∏Å‡∏≤‡∏£']):
            metadata['type'] = 'medical'

        important_words = [word for word in text_lower.split()
                           if len(word) > 3 and not word.isdigit()]
        metadata['keywords'] = set(important_words[:10])

        numbers = re.findall(r'\d+(?:,\d+)*(?:\.\d+)?', section)
        metadata['numbers'] = [float(num.replace(',', '')) for num in numbers]

        return metadata

    def split_into_logical_sections(self, text):
        sections = []
        current_section = ""

        for line in text.split('\n'):
            line = line.strip()
            if not line:
                continue

            if (re.match(r'^\d+\.', line) or
                    re.match(r'^[‡∏Å-‡∏Æ]\.', line) or
                    line.isupper() or
                    any(line.startswith(prefix) for prefix in ['‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á', '‡∏ö‡∏ó‡∏ó‡∏µ‡πà', '‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà'])):

                if current_section:
                    sections.append(current_section)
                current_section = line
            else:
                current_section += "\n" + line

        if current_section:
            sections.append(current_section)

        return sections

    def process_pdf_content(self):
        self.content_sections = []
        self.document_categories = {}

        for filename in os.listdir(self.pdf_folder):
            if filename.endswith('.pdf'):
                try:
                    file_path = os.path.join(self.pdf_folder, filename)
                    with open(file_path, 'rb') as file:
                        reader = PyPDF2.PdfReader(file)
                        full_text = ""

                        for page in reader.pages:
                            full_text += page.extract_text() + "\n"

                        categories = self.categorize_document(full_text, filename)
                        self.document_categories[filename] = categories

                        sections = self.split_into_logical_sections(full_text)

                        for section in sections:
                            if len(section.strip()) > 10:
                                metadata = self.extract_section_metadata(section)
                                self.content_sections.append({
                                    'content': section.strip(),
                                    'source': filename,
                                    'categories': categories,
                                    'metadata': metadata
                                })

                        logging.info(f"‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå {filename} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {categories})")

                except Exception as e:
                    logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå {filename}: {e}")

    def find_relevant_content(self, query, max_sections=3):
        query_lower = query.lower()
        scored_sections = []

        query_words = set(query_lower.split())

        for section in self.content_sections:
            score = 0
            content_lower = section['content'].lower()
            metadata = section['metadata']

            matching_keywords = query_words.intersection(metadata['keywords'])
            score += len(matching_keywords) * 2

            for word in query_words:
                if len(word) > 3 and word in content_lower:
                    score += 1

            if ('‡∏£‡∏≤‡∏Ñ‡∏≤' in query_lower or '‡∏Ñ‡πà‡∏≤' in query_lower) and metadata['has_pricing']:
                score += 3

            if score > 0:
                scored_sections.append((score, section))

        scored_sections.sort(key=lambda x: x[0], reverse=True)
        return [section for score, section in scored_sections[:max_sections]]

    def create_response_context(self, query, relevant_sections):
        context_parts = []
        for section in relevant_sections:
            source = section['source']
            categories = ', '.join(section['categories'])
            context_parts.append(f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {source} (‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {categories}):\n{section['content']}")

        context = "\n\n".join(context_parts)

        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

        ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}

        ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:
        {context}

        ‡∏Å‡∏é‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
        1. ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        2. ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏±‡∏ô‡∏ô‡∏¥‡∏©‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        3. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"
        4. ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏°‡∏≠
        5. ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤ ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ
        6. ‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ üåª

        ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:"""

        return prompt

    def save_content(self):
        os.makedirs("knowledge_base", exist_ok=True)
        data = {
            'categories': self.document_categories,
            'sections': [{
                'content': section['content'],
                'source': section['source'],
                'categories': section['categories'],
                'metadata': {
                    'type': section['metadata']['type'],
                    'has_pricing': section['metadata']['has_pricing'],
                    'numbers': section['metadata']['numbers']
                }
            } for section in self.content_sections]
        }
        with open("knowledge_base/processed_content.json", "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logging.info("‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

    def load_or_create_content(self):
        try:
            with open("knowledge_base/processed_content.json", "r", encoding="utf-8") as f:
                data = json.load(f)
                self.document_categories = data['categories']
                self.content_sections = data['sections']
                for section in self.content_sections:
                    section['metadata']['keywords'] = set()  # Initialize empty set for keywords
            logging.info("‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        except:
            logging.info("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏´‡∏°‡πà...")
            self.process_pdf_content()
            self.save_content()

    def update_content(self):
        self.process_pdf_content()
        self.save_content()
        return "‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå PDF ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß üåª"


import logging
import vertexai
from vertexai.language_models import TextGenerationModel


class VertexAIHandler:
    def __init__(self):
        try:
            # Initialize Vertex AI
            vertexai.init(
                project="lexical-period-444405-e3",
                location="us-central1"
            )

            # Initialize the text model
            self.model = TextGenerationModel.from_pretrained("text-bison@002")
            logging.info("VertexAI Handler initialized successfully")

        except Exception as e:
            logging.error(f"Failed to initialize VertexAI Handler: {e}")
            raise

    def generate_response(self, prompt):
        try:
            response = self.model.predict(
                prompt,
                temperature=0.3,
                max_output_tokens=8192,
                top_k=40,
                top_p=0.8,
            )
            return response.text
        except Exception as e:
            logging.error(f"Error generating response: {e}")
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á üåª"


# Initialize Flask and LINE
app = Flask(__name__)
configuration = Configuration(access_token=LINE_CHANNEL_ACCESS_TOKEN)
handler = WebhookHandler(LINE_CHANNEL_SECRET)
api_client = ApiClient(configuration)
line_bot_api = MessagingApi(api_client)

# Initialize Content Manager and Vertex AI Handler
content_manager = PDFContentManager()
vertex_handler = VertexAIHandler()


@app.route("/callback", methods=['POST'])
def callback():
    signature = request.headers['X-Line-Signature']
    body = request.get_data(as_text=True)
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        abort(400)
    return 'OK'


@handler.add(MessageEvent, message=TextMessageContent)
def handle_message(event):
    user_id = event.source.user_id
    user_question = event.message.text
    logging.info(f"Received message from {user_id}: {user_question}")

    try:
        # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏≠‡∏Å‡πà‡∏≠‡∏ô
        line_bot_api.reply_message_with_http_info(
            ReplyMessageRequest(
                reply_token=event.reply_token,
                messages=[TextMessage(text="‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...")]
            )
        )

        if user_question.strip().lower() == "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å pdf ‡πÉ‡∏´‡∏°‡πà‡∏´‡∏ô‡πà‡∏≠‡∏¢":
            response = content_manager.update_content()
        else:
            relevant_sections = content_manager.find_relevant_content(user_question)
            if not relevant_sections:
                response = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ üåª"
            else:
                prompt = content_manager.create_response_context(user_question, relevant_sections)
                response = vertex_handler.generate_response(prompt)

        line_bot_api.push_message_with_http_info(
            PushMessageRequest(
                to=user_id,
                messages=[TextMessage(text=response)]
            )
        )

    except Exception as e:
        logging.error(f"Error handling message: {e}")
        line_bot_api.push_message_with_http_info(
            PushMessageRequest(
                to=user_id,
                messages=[TextMessage(text="‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• üåª")]
            )
        )


if __name__ == "__main__":
    app.run(host='0.0.0.0', port=8080)